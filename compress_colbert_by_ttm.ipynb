{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebook/ColBERT\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing ColBERT: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing ColBERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ColBERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ColBERT were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['linear.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109580544\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/notebook/ColBERT\")\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AdamW\n",
    "from colbert.utils.runs import Run\n",
    "from colbert.utils.amp import MixedPrecisionManager\n",
    "\n",
    "from colbert.training.lazy_batcher import LazyBatcher\n",
    "from colbert.training.eager_batcher import EagerBatcher\n",
    "from colbert.parameters import DEVICE\n",
    "\n",
    "from colbert.modeling.colbert import ColBERT\n",
    "from colbert.utils.utils import print_message\n",
    "from colbert.training.utils import print_progress, manage_checkpoints\n",
    "\n",
    "from colbert.utils.tensor_net import TTLayer\n",
    "\n",
    "query_maxlen = 512\n",
    "query_maxlen = 512\n",
    "doc_maxlen = 512\n",
    "dim = 128\n",
    "similarity = 'cosine'\n",
    "\n",
    "colbert = ColBERT.from_pretrained('bert-base-uncased', query_maxlen=query_maxlen, doc_maxlen=doc_maxlen, dim=dim, similarity_metric=similarity, mask_punctuation=False)\n",
    "print(sum(p.numel() for p in colbert.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer.shape torch.Size([3072, 768])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([32, 48, 390])\n",
      "torch.Size([390, 48, 32])\n",
      "torch.Size([32, 32, 1])\n",
      "layer.shape torch.Size([768, 3072])\n",
      "layer.shape torch.Size([3072, 768])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([32, 48, 390])\n",
      "torch.Size([390, 48, 32])\n",
      "torch.Size([32, 32, 1])\n",
      "layer.shape torch.Size([768, 3072])\n",
      "layer.shape torch.Size([3072, 768])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([32, 48, 390])\n",
      "torch.Size([390, 48, 32])\n",
      "torch.Size([32, 32, 1])\n",
      "layer.shape torch.Size([768, 3072])\n",
      "layer.shape torch.Size([3072, 768])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([32, 48, 390])\n",
      "torch.Size([390, 48, 32])\n",
      "torch.Size([32, 32, 1])\n",
      "layer.shape torch.Size([768, 3072])\n",
      "layer.shape torch.Size([3072, 768])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([32, 48, 390])\n",
      "torch.Size([390, 48, 32])\n",
      "torch.Size([32, 32, 1])\n",
      "layer.shape torch.Size([768, 3072])\n",
      "layer.shape torch.Size([3072, 768])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([32, 48, 390])\n",
      "torch.Size([390, 48, 32])\n",
      "torch.Size([32, 32, 1])\n",
      "layer.shape torch.Size([768, 3072])\n",
      "95670528\n"
     ]
    }
   ],
   "source": [
    "TT_SHAPES = (32, 48, 48, 32)\n",
    "TT_RANKS = [1, 380, 390, 380, 1] # comp rate 0.5\n",
    "SVD_RANKS = 350\n",
    "\n",
    "\n",
    "from tensor_net1 import TTLayer\n",
    "for i in [0, 2, 4, 6, 8, 10]:\n",
    "            # fc part\n",
    "            fc_w = colbert.bert.encoder.layer[i].intermediate.dense\n",
    "            fc_b = colbert.bert.encoder.layer[i].intermediate.dense.bias\n",
    "            (out_, in_) = fc_w.weight.shape\n",
    "            factorized_layer = TTLayer(fc_w, shapes = TT_SHAPES, in_dims = [32, 24], ranks = TT_RANKS)\n",
    "            for elem in factorized_layer.cores:\n",
    "                print (elem.shape)\n",
    "\n",
    "            colbert.bert.encoder.layer[i].intermediate.dense = factorized_layer\n",
    "\n",
    "            fc_w = colbert.bert.encoder.layer[i].output.dense\n",
    "            factorized_layer = TTLayer(fc_w, shapes = TT_SHAPES, in_dims = [32, 48, 2], ranks = TT_RANKS)\n",
    "            colbert.bert.encoder.layer[i].output.dense = factorized_layer\n",
    "\n",
    "print(sum(p.numel() for p in colbert.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer.shape torch.Size([3072, 768])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([32, 48, 450])\n",
      "torch.Size([450, 48, 32])\n",
      "torch.Size([32, 32, 1])\n",
      "layer.shape torch.Size([768, 3072])\n",
      "layer.shape torch.Size([3072, 768])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([32, 48, 450])\n",
      "torch.Size([450, 48, 32])\n",
      "torch.Size([32, 32, 1])\n",
      "layer.shape torch.Size([768, 3072])\n",
      "layer.shape torch.Size([3072, 768])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([32, 48, 450])\n",
      "torch.Size([450, 48, 32])\n",
      "torch.Size([32, 32, 1])\n",
      "layer.shape torch.Size([768, 3072])\n",
      "layer.shape torch.Size([3072, 768])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([32, 48, 450])\n",
      "torch.Size([450, 48, 32])\n",
      "torch.Size([32, 32, 1])\n",
      "layer.shape torch.Size([768, 3072])\n",
      "layer.shape torch.Size([3072, 768])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([32, 48, 450])\n",
      "torch.Size([450, 48, 32])\n",
      "torch.Size([32, 32, 1])\n",
      "layer.shape torch.Size([768, 3072])\n",
      "layer.shape torch.Size([3072, 768])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([32, 48, 450])\n",
      "torch.Size([450, 48, 32])\n",
      "torch.Size([32, 32, 1])\n",
      "layer.shape torch.Size([768, 3072])\n",
      "97882368\n"
     ]
    }
   ],
   "source": [
    "TT_SHAPES = (32, 48, 48, 32)\n",
    "TT_RANKS = [1, 450, 450, 450, 1] # comp rate 0.5\n",
    "SVD_RANKS = 350\n",
    "\n",
    "\n",
    "from tensor_net1 import TTLayer\n",
    "for i in [0, 2, 4, 6, 8, 10]:\n",
    "            # fc part\n",
    "            fc_w = colbert.bert.encoder.layer[i].intermediate.dense\n",
    "            fc_b = colbert.bert.encoder.layer[i].intermediate.dense.bias\n",
    "            (out_, in_) = fc_w.weight.shape\n",
    "            factorized_layer = TTLayer(fc_w, shapes = TT_SHAPES, in_dims = [32, 24], ranks = TT_RANKS)\n",
    "            for elem in factorized_layer.cores:\n",
    "                print (elem.shape)\n",
    "\n",
    "            colbert.bert.encoder.layer[i].intermediate.dense = factorized_layer\n",
    "\n",
    "            fc_w = colbert.bert.encoder.layer[i].output.dense\n",
    "            factorized_layer = TTLayer(fc_w, shapes = TT_SHAPES, in_dims = [32, 48, 2], ranks = TT_RANKS)\n",
    "            colbert.bert.encoder.layer[i].output.dense = factorized_layer\n",
    "\n",
    "print(sum(p.numel() for p in colbert.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.rand(256, 142, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape torch.Size([256, 142, 768])\n",
      "shapes, in_dims, ranks (32, 48, 48, 32) [32, 24] [1, 380, 390, 380, 1]\n",
      "256 142\n",
      "result.shape, core.shape torch.Size([36352, 32, 24]) torch.Size([32, 32])\n",
      "result.shape torch.Size([36352, 24, 32])\n",
      "result.shape, core.shape torch.Size([36352, 24, 32]) torch.Size([32, 24, 2, 390])\n",
      "result.shape torch.Size([36352, 2, 390])\n",
      "result.shape, core.shape torch.Size([256, 284, 390]) torch.Size([390, 48, 32])\n",
      "result.shape torch.Size([256, 284, 48, 32])\n",
      "result.shape, core.shape torch.Size([256, 13632, 32]) torch.Size([32, 32, 1])\n",
      "result.shape torch.Size([256, 13632, 32, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 142, 3072])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = factorized_layer(v)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "0-dimensional array given. Array must be at least two-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_83532/4025707279.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfc_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# truncate SVD and fuse Sigma matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mw1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msvd\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/opt/.pyenv/versions/3.8.10/lib/python3.8/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, hermitian)\u001b[0m\n\u001b[1;32m   1640\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1642\u001b[0;31m     \u001b[0m_assert_stacked_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1643\u001b[0m     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_commonType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/.pyenv/versions/3.8.10/lib/python3.8/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36m_assert_stacked_2d\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             raise LinAlgError('%d-dimensional array given. Array must be '\n\u001b[0m\u001b[1;32m    198\u001b[0m                     'at least two-dimensional' % a.ndim)\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLinAlgError\u001b[0m: 0-dimensional array given. Array must be at least two-dimensional"
     ]
    }
   ],
   "source": [
    "U, S, Vt = np.linalg.svd(fc_w, full_matrices=False)\n",
    "\n",
    "\n",
    "# truncate SVD and fuse Sigma matrix\n",
    "w1 = np.dot(np.diag(np.sqrt(S[0:rank])),Vt[0:rank, :])\n",
    "w2 = np.dot(U[:, 0:rank], np.diag(np.sqrt(S[0:rank])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109580544\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in colbert.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from colbert.utils.easytt import TTLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 48, 48, 32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32, 48, 48, 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 128, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "import tntorch as tn\n",
    "import torch\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "X, Y, Z, W = np.meshgrid(range(128), range(128), range(128), range(128))\n",
    "full = torch.randn((128, 128, 128, 128))  # Some analytical 3D function\n",
    "print(full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/.pyenv/versions/3.8.10/lib/python3.8/site-packages/tensorly/backend/core.py:1106: UserWarning: In partial_svd: converting to NumPy. Check SVD_FUNS for available alternatives if you want to avoid this.\n",
      "  warnings.warn('In partial_svd: converting to NumPy.'\n"
     ]
    }
   ],
   "source": [
    "import tensorly as tl\n",
    "from tensorly.decomposition import tensor_train\n",
    "tl.set_backend('pytorch')\n",
    "\n",
    "t = tensor_train(full, rank = [1, 130, 130, 130, 1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = [[32, 48], [48, 32]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes [[32 48]\n",
      " [48 32]]\n",
      "[32, 48, 48, 32]\n",
      "torch.Size([32, 48, 48, 32])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35181/635488875.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mranks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m530\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m540\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m530\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mranks_tt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mranks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/.pyenv/versions/3.8.10/lib/python3.8/site-packages/tntorch/tensor.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, Us, idxs, device, requires_grad, ranks_cp, ranks_tucker, ranks_tt, eps, max_iter, tol, verbose, batch, algorithm)\u001b[0m\n\u001b[1;32m    264\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround_tucker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mranks_tucker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mranks_tt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround_tt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mranks_tt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;31m# Check factor shapes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/.pyenv/versions/3.8.10/lib/python3.8/site-packages/tntorch/tensor.py\u001b[0m in \u001b[0;36mround_tt\u001b[0;34m(self, eps, rmax, algorithm, verbose)\u001b[0m\n\u001b[1;32m   1667\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m             \u001b[0mrmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrmax\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrmax\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cp_to_tt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "shapes = np.asarray(shapes)\n",
    "print (\"shapes\", shapes)\n",
    "print (list(shapes.flatten()))\n",
    "matrix = colbert.bert.encoder.layer[1].intermediate.dense.weight.reshape(list(shapes.flatten()))\n",
    "print (matrix.shape)  \n",
    "d = len(shapes[0])\n",
    "transpose_idx = list(np.arange(2 * d).reshape(2, d).T.flatten())\n",
    "matrix = matrix.permute(*transpose_idx)\n",
    "newshape = np.prod(shapes, 0)\n",
    "matrix = matrix.reshape(list(newshape))\n",
    "ranks = [530, 540, 530]\n",
    "tt = tn.Tensor(matrix, ranks_tt=ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(np.arange(2 * 1).reshape(2, 1).T.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'in_modes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3275/2513253187.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTTLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_modes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m48\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_modes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m48\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mranks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m530\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m540\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m530\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'in_modes'"
     ]
    }
   ],
   "source": [
    "print (\"matrix.shapes\", matrix.shape)\n",
    "shapes = np.asarray(shapes)\n",
    "print (\"shapes\", shapes)\n",
    "print (list(shapes.flatten()))\n",
    "matrix = matrix.reshape(list(shapes.flatten()))\n",
    "print (matrix.shape)  \n",
    "d = len(shapes[0])\n",
    "transpose_idx = list(np.arange(2 * d).reshape(2, d).T.flatten())\n",
    "matrix = matrix.permute(*transpose_idx)\n",
    "newshape = np.prod(shapes, 0)\n",
    "matrix = matrix.reshape(list(newshape))\n",
    "tt = tn.Tensor(matrix, ranks_tt=ranks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import tntorch as tn\n",
    "\n",
    "\n",
    "def matrix_to_tt_cores(matrix, shapes, ranks):\n",
    "  print (\"matrix.shapes\", matrix.shape)\n",
    "  shapes = np.asarray(shapes)\n",
    "  print (\"shapes\", shapes)\n",
    "  print (list(shapes.flatten()))\n",
    "  matrix = matrix.reshape(list(shapes.flatten()))\n",
    "  print (matrix.shape)  \n",
    "  d = len(shapes[0])\n",
    "  transpose_idx = list(np.arange(2 * d).reshape(2, d).T.flatten())\n",
    "  matrix = matrix.permute(*transpose_idx)\n",
    "  newshape = np.prod(shapes, 0)\n",
    "  matrix = matrix.reshape(list(newshape))\n",
    "  #tt = tn.Tensor(matrix, ranks_tt=ranks)\n",
    "  tt = tensor_train(full, rank = ranks)\n",
    "\n",
    "\n",
    "  newcores = []\n",
    "  for core, s1, s2, r1, r2 in zip(tt.cores,\n",
    "                                  shapes[0], shapes[1],\n",
    "                                  tt.ranks_tt, tt.ranks_tt[1:]):\n",
    "    newcores.append(core.reshape((r1, s1, s2, r2)))\n",
    "  return newcores\n",
    "\n",
    "\n",
    "def ttmatmul(cores, t, shapes, ranks):\n",
    "  ranks = [1] + ranks + [1]\n",
    "  tshape = t.shape\n",
    "\n",
    "  t = t.transpose(1, 0)\n",
    "  t = t.reshape((-1, shapes[1][-1], 1))\n",
    "  ndims = len(cores)\n",
    "  for i in reversed(range(ndims)):\n",
    "    t = torch.einsum('aijb,rjb->ira', (cores[i], t))\n",
    "    if i:\n",
    "      t = t.reshape((-1, shapes[1][i - 1], ranks[i]))\n",
    "  t = t.reshape((int(np.prod(shapes[0])), tshape[1]))\n",
    "  return t\n",
    "\n",
    "\n",
    "def transpose(cores):\n",
    "  result = []\n",
    "  for c in cores:\n",
    "    result.append(c.permute((0, 2, 1, 3)))\n",
    "  return result\n",
    "\n",
    "\n",
    "def matmultt(t, cores, shapes, ranks):\n",
    "  t = t.transpose(1, 0)\n",
    "  cores = transpose(cores)\n",
    "  shapes = [shapes[1], shapes[0]]\n",
    "  return ttmatmul(cores, t, shapes, ranks).transpose(1, 0)\n",
    "\n",
    "\n",
    "class TTLayer(nn.Module):\n",
    "  def __init__(self, layer, shapes, ranks):\n",
    "    super(TTLayer, self).__init__()\n",
    "    self.shapes = shapes\n",
    "    self.ranks = ranks\n",
    "    with torch.no_grad():\n",
    "      weight = layer.weight.transpose(1, 0).data.cpu()\n",
    "      print (\"weight shape\", weight.shape)\n",
    "      self.cores = nn.ParameterList(\n",
    "          map(nn.Parameter, matrix_to_tt_cores(weight, shapes, ranks)))\n",
    "    self.bias = layer.bias\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    out = matmultt(inputs, self.cores, self.shapes, self.ranks)\n",
    "    out = out + self.bias\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classes.gpt2_tt import GPT2_TT_Model\n",
    "from src.classes.gpt_med_config import GPT2MedConfig\n",
    "\n",
    "rank = 70\n",
    "\n",
    "for i in [0, 2, 4, 6, 8, 10]:\n",
    "    # fc part\n",
    "    fc_w = colbert.bert.encoder.layer[i].intermediate.dense\n",
    "    print (fc_w.weight.shape)\n",
    "    fc_b = colbert.bert.encoder.layer[i].intermediate.dense.bias.data.cpu()\n",
    "    factorized_layer = TTLayer(fc_w, shapes = [[32, 48], [48, 32]], ranks = [1, 530, 540, 530, 1])\n",
    "    print (len(factorized_layer.ttm.tt.cores))\n",
    "    for elem in factorized_layer.ttm.tt.cores:\n",
    "        print (elem.shape)\n",
    "    colbert.bert.encoder.layer[i].intermediate.dense = factorized_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "TTM-Linear required dimensions: dim_in=3072, dim_out=768, rank=50, max_dim=50\n",
      "    after best_approx: dim_in=3072, dim_out=768\n",
      "    dim_in factorization:  (2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3)\n",
      "    dim_out factorization: (2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3)\n",
      "    dims before shrink:  [(2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 3), (2, 1), (3, 1)]\n",
      "    final TTM dims:  [(8, 8), (8, 8), (8, 12), (6, 1)]\n",
      "ranks [1, 50, 50, 50, 1]\n",
      "dims 1 8 8 50 3200\n",
      "dims 50 8 8 50 160000\n",
      "dims 50 8 12 50 240000\n",
      "dims 50 6 1 1 300\n",
      "self.dims [(8, 8), (8, 8), (8, 12), (6, 1)]\n",
      "ranks in TTM [50, 50, 50]\n",
      "    Original linear params: 2359296, ttm params: 320000 (x0.136)\n",
      "-------------------------------------\n",
      "torch.Size([8, 8, 8, 6, 8, 8, 12, 1]) torch.Size([8, 8, 8, 6, 8, 8, 12, 1])\n",
      "tensor(1.9725)\n",
      "4\n",
      "torch.Size([1, 8, 8, 50])\n",
      "torch.Size([50, 8, 8, 50])\n",
      "torch.Size([50, 8, 12, 6])\n",
      "torch.Size([6, 6, 1, 1])\n",
      "self.dims [(8, 8), (8, 8), (8, 12), (6, 1)]\n",
      "ranks in TTM [50, 50, 50]\n",
      "    Original linear params: 2359296, ttm params: 320000 (x0.136)\n",
      "-------------------------------------\n",
      "torch.Size([8, 8, 8, 6, 8, 8, 12, 1]) torch.Size([8, 8, 8, 6, 8, 8, 12, 1])\n",
      "tensor(2.0527)\n",
      "4\n",
      "torch.Size([1, 8, 8, 50])\n",
      "torch.Size([50, 8, 8, 50])\n",
      "torch.Size([50, 8, 12, 6])\n",
      "torch.Size([6, 6, 1, 1])\n",
      "self.dims [(8, 8), (8, 8), (8, 12), (6, 1)]\n",
      "ranks in TTM [50, 50, 50]\n",
      "    Original linear params: 2359296, ttm params: 320000 (x0.136)\n",
      "-------------------------------------\n",
      "torch.Size([8, 8, 8, 6, 8, 8, 12, 1]) torch.Size([8, 8, 8, 6, 8, 8, 12, 1])\n",
      "tensor(2.1566)\n",
      "4\n",
      "torch.Size([1, 8, 8, 50])\n",
      "torch.Size([50, 8, 8, 50])\n",
      "torch.Size([50, 8, 12, 6])\n",
      "torch.Size([6, 6, 1, 1])\n",
      "self.dims [(8, 8), (8, 8), (8, 12), (6, 1)]\n",
      "ranks in TTM [50, 50, 50]\n",
      "    Original linear params: 2359296, ttm params: 320000 (x0.136)\n",
      "-------------------------------------\n",
      "torch.Size([8, 8, 8, 6, 8, 8, 12, 1]) torch.Size([8, 8, 8, 6, 8, 8, 12, 1])\n",
      "tensor(2.2240)\n",
      "4\n",
      "torch.Size([1, 8, 8, 50])\n",
      "torch.Size([50, 8, 8, 50])\n",
      "torch.Size([50, 8, 12, 6])\n",
      "torch.Size([6, 6, 1, 1])\n",
      "self.dims [(8, 8), (8, 8), (8, 12), (6, 1)]\n",
      "ranks in TTM [50, 50, 50]\n",
      "    Original linear params: 2359296, ttm params: 320000 (x0.136)\n",
      "-------------------------------------\n",
      "torch.Size([8, 8, 8, 6, 8, 8, 12, 1]) torch.Size([8, 8, 8, 6, 8, 8, 12, 1])\n",
      "tensor(2.2022)\n",
      "4\n",
      "torch.Size([1, 8, 8, 50])\n",
      "torch.Size([50, 8, 8, 50])\n",
      "torch.Size([50, 8, 12, 6])\n",
      "torch.Size([6, 6, 1, 1])\n",
      "self.dims [(8, 8), (8, 8), (8, 12), (6, 1)]\n",
      "ranks in TTM [50, 50, 50]\n",
      "    Original linear params: 2359296, ttm params: 320000 (x0.136)\n",
      "-------------------------------------\n",
      "torch.Size([8, 8, 8, 6, 8, 8, 12, 1]) torch.Size([8, 8, 8, 6, 8, 8, 12, 1])\n",
      "tensor(2.1815)\n",
      "4\n",
      "torch.Size([1, 8, 8, 50])\n",
      "torch.Size([50, 8, 8, 50])\n",
      "torch.Size([50, 8, 12, 6])\n",
      "torch.Size([6, 6, 1, 1])\n",
      "96563160\n"
     ]
    }
   ],
   "source": [
    "from src.classes.gpt2_tt import GPT2_TT_Model\n",
    "from src.layers2.linear import TTMLinear\n",
    "from src.classes.gpt_med_config import GPT2MedConfig\n",
    "\n",
    "from src.ttm_linear.ttm_linear.ttm_linear import FactorizationTTMLinear\n",
    "rank = 50\n",
    "\n",
    "for i in [0, 2, 4, 6, 8, 10]:\n",
    "    # fc part\n",
    "    fc_w = colbert.bert.encoder.layer[i].intermediate.dense.weight.data\n",
    "    fc_b = colbert.bert.encoder.layer[i].intermediate.dense.bias.data\n",
    "    (in_, out_) = fc_w.shape\n",
    "    factorized_layer = FactorizationTTMLinear(in_, out_, rank=rank, max_core_dim_product = rank)\n",
    "    factorized_layer.fill_with_pretrained_matrix(fc_w, reshape_sizes = (8, 12, 8, 16, 12, 16))\n",
    "    print (len(factorized_layer.ttm.tt.cores))\n",
    "    for elem in factorized_layer.ttm.tt.cores:\n",
    "        print (elem.shape)\n",
    "    colbert.bert.encoder.layer[i].intermediate.dense = factorized_layer\n",
    "print(sum(p.numel() for p in colbert.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99752064\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in colbert.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
