{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-08 20:48:40.893162: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing ColBERT: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing ColBERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ColBERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ColBERT were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['linear.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AdamW\n",
    "from colbert.utils.runs import Run\n",
    "from colbert.utils.amp import MixedPrecisionManager\n",
    "\n",
    "from colbert.training.lazy_batcher import LazyBatcher\n",
    "from colbert.training.eager_batcher import EagerBatcher\n",
    "from colbert.parameters import DEVICE\n",
    "\n",
    "from colbert.modeling.colbert import ColBERT\n",
    "from colbert.utils.utils import print_message\n",
    "from colbert.training.utils import print_progress, manage_checkpoints\n",
    "\n",
    "query_maxlen = 512\n",
    "query_maxlen = 512\n",
    "doc_maxlen = 512\n",
    "dim = 128\n",
    "similarity = 'cosine'\n",
    "\n",
    "colbert = ColBERT.from_pretrained('bert-base-uncased', query_maxlen=query_maxlen, doc_maxlen=doc_maxlen, dim=dim, similarity_metric=similarity, mask_punctuation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colbert.utils.easytt import TTLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109580544\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in colbert.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colbert.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109580544\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in colbert.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3072, 768)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colbert.bert.encoder.layer[5].intermediate.dense.weight.data.cpu().data.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_utils import Conv1D\n",
    "\n",
    "def factorize_to_svd(fc_w, fc_b, rank, device = 'cpu'):\n",
    "    U, S, Vt = np.linalg.svd(fc_w, full_matrices=False)\n",
    "\n",
    "\n",
    "    # truncate SVD and fuse Sigma matrix\n",
    "    w1 = np.dot(np.diag(np.sqrt(S[0:rank])),Vt[0:rank, :])\n",
    "    w2 = np.dot(U[:, 0:rank], np.diag(np.sqrt(S[0:rank])))\n",
    "\n",
    "    # create new layers and insert weights\n",
    "    out_features, in_features = fc_w.shape\n",
    "    linear1 = nn.Linear(in_features = in_features, \n",
    "                          out_features = rank,\n",
    "                          bias = False)\n",
    "    linear1.weight = nn.Parameter(torch.FloatTensor(w1))\n",
    "\n",
    "    linear2 = nn.Linear(in_features = rank,\n",
    "                          out_features = out_features,\n",
    "                          bias=True)\n",
    "    linear2.weight = nn.Parameter(torch.FloatTensor(w2))\n",
    "    linear2.bias = nn.Parameter(torch.FloatTensor(fc_b))\n",
    "\n",
    "    # create factorized layer\n",
    "    factorized_layer = nn.Sequential(linear1, linear2)\n",
    "    \n",
    "    print (linear1.weight.shape, linear2.weight.shape)\n",
    "    \n",
    "    return factorized_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 768]) torch.Size([3072, 70])\n"
     ]
    }
   ],
   "source": [
    "in_ = 768\n",
    "out_ = 3072\n",
    "batch_size = 128\n",
    "rank = 16\n",
    "fc_w = colbert.bert.encoder.layer[2].intermediate.dense.weight.data.cpu().data.numpy()\n",
    "fc_b = colbert.bert.encoder.layer[2].intermediate.dense.bias.data.cpu().data.numpy()\n",
    "m = nn.Linear(in_, out_)\n",
    "input_ = torch.randn(batch_size, in_)\n",
    "m_ttm = factorize_to_svd(fc_w, fc_b,rank=70)\n",
    "\n",
    "out1 = m(input_)\n",
    "out2 = m_ttm(input_)\n",
    "assert out1.shape == torch.squeeze(out2).shape\n",
    "assert torch.squeeze(out2).shape == (batch_size, out_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 768]) torch.Size([3072, 40])\n",
      "torch.Size([40, 768]) torch.Size([3072, 40])\n",
      "torch.Size([40, 768]) torch.Size([3072, 40])\n",
      "torch.Size([40, 768]) torch.Size([3072, 40])\n",
      "torch.Size([40, 768]) torch.Size([3072, 40])\n",
      "torch.Size([40, 768]) torch.Size([3072, 40])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "for i in [0, 2, 4, 6, 8, 10]:\n",
    "    # fc part\n",
    "    fc_w = colbert.bert.encoder.layer[i].intermediate.dense.weight.data.cpu().data.numpy()\n",
    "    fc_b = colbert.bert.encoder.layer[i].intermediate.dense.bias.data.cpu().data.numpy()\n",
    "    factorized_layer = factorize_to_svd(fc_w, fc_b, rank = 40)\n",
    "    colbert.bert.encoder.layer[i].intermediate.dense = factorized_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96346368\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in colbert.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to %s /notebook/ColBERT/compressed_checkpoint\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"/notebook/ColBERT/compressed_checkpoint\"\n",
    "model_to_save = colbert.module if hasattr(colbert, 'module') else colbert  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "torch.save(colbert.state_dict(), os.path.join(output_dir, 'model_tt.pth'))\n",
    "print(\"Saving model checkpoint to %s\", output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97498368\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in colbert.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colbert.num_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/notebook/greenAI/src/ttm_linear/ttm_linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-2d3fba514036>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mttm_linear\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFactorizationTTMLinear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# fc part\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mfc_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfc_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebook/greenAI/src/ttm_linear/ttm_linear/ttm_linear.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfull_matrix_backward\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfull_matrix_backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mttm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_backward_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meinsum_forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from ttm_linear import FactorizationTTMLinear\n",
    "for i in [0, 2, 4, 6, 8, 10]:\n",
    "    # fc part\n",
    "    fc_w = colbert.bert.encoder.layer[i].intermediate.dense.weight.data.cpu().data.numpy()\n",
    "    fc_b = colbert.bert.encoder.layer[i].intermediate.dense.bias.data.cpu().data.numpy()\n",
    "    (in_, out_) = fc_w.shape\n",
    "    factorized_layer = FactorizationTTMLinear(in_, out_, rank=rank, max_core_dim_product = rank)\n",
    "    colbert.bert.encoder.layer[i].intermediate.dense = factorized_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy TN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/notebook/GreedyTN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "from argparse import RawTextHelpFormatter\n",
    "from ast import literal_eval\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import discrete_optim_tensor_decomposition\n",
    "from random_tensors import *\n",
    "from tensor_decomposition_models import incremental_tensor_decomposition\n",
    "from utils import seed_everything\n",
    "from utils import tic, toc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_dic = {'tucker': generate_tucker, 'tt': generate_tensor_train, 'tr': generate_tensor_ring, 'triangle': generate_tensor_tri}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 3072])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#goal_tn = gen_dic[target_type](target_dims, target_rank)\n",
    "\n",
    "target_full = matrix.reshape((128, 144, 128))\n",
    "print (target_full.shape)\n",
    "target_full = target_full / torch.norm(target_full)\n",
    "result = {'target_tn': [], 'target_full': target_full}\n",
    "\n",
    "for decomp in \"TT\".split():\n",
    "    print(decomp + \"...\")\n",
    "    tic()\n",
    "    result[decomp] = incremental_tensor_decomposition(target_full, decomp, verbose=False, max_num_params=2000000,\n",
    "                                                              rank_increment_factor=1.5 if decomp == 'CP' else 1)\n",
    "    result[decomp + \"-time\"] = toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_full = matrix.reshape((16, 32, 16, 16, 18))\n",
    "print (target_full.shape)\n",
    "target_full = target_full / torch.norm(target_full)\n",
    "result = {'target_tn': [], 'target_full': target_full}\n",
    "\n",
    "for decomp in \"TT\".split():\n",
    "    print(decomp + \"...\")\n",
    "    tic()\n",
    "    result[decomp] = incremental_tensor_decomposition(target_full, decomp, verbose=False, max_num_params=2000000,\n",
    "                                                              rank_increment_factor=1.5 if decomp == 'CP' else 1)\n",
    "    result[decomp + \"-time\"] = toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decomposed params: 9216 + 2359296 + "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all params: 2359296"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
